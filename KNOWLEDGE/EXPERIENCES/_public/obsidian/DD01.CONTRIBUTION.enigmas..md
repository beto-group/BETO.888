


### Overview of Datacore System and Challenges

The Datacore system is a **document query engine** designed to handle **block, section, and page-level queries** over metadata. It also has a built-in caching system that allows for efficient retrieval and management of document information. This project is in development, with its primary focus being **DatacoreJS**, a **JavaScript API** aimed at power users to create high-level views. The system also supports JSX and TypeScript via the `sucrase` transpiler.

Despite significant progress, there are several ongoing challenges and areas for development:

### Key Features Under Development

1. **Editable Components**:
    
    - There is metadata stored in the indices that makes it possible to implement editable components via a `Field` abstraction. However, the function to update the field with a new value is still unimplemented.
    - The system needs React components to render an interactive edit box. Additionally, any update system must ensure user changes are preserved during concurrent updates.
2. **Fast Embedding**:
    
    - While there is logic for **native Obsidian embedding** of sections and tagged blocks, the embedding of other elements such as blocks via line numbers is not fully exposed in the API.
    - This could be abstracted further for flexibility.
3. **Visual Components**:
    
    - The goal is to create **universal visual components** such as buttons, textboxes, and dropdown selectors. These will help users create interactive views.
4. **Performance**:
    
    - **Datacore**'s performance, while generally acceptable, faces issues with **initial indexing**, which is slower than that of **Dataview**. Despite this slowdown, performance graphs reveal substantial idle time, indicating inefficiency in the current implementation.
5. **Pre-built "Batteries Included" Views**:
    
    - Some standard views, like **table views**, **image galleries**, **card views**, and **list views**, are planned for easier interaction.
    - Chart views are also considered, but introducing heavy dependencies like **D3.js** could be problematic.
6. **Query Language Improvements**:
    
    - Although the query language closely resembles **Dataview's**, there are planned refinements, such as adding **infix functions** and an **in operator** similar to Python.

---

### Current Problems and Solutions

#### 1. Batch Processing for Speed Improvement

A significant issue involves performance, specifically the inefficiency during import processes due to pauses between each import. One proposed solution is batching the imports in **groups of 8-16**, which could dramatically increase speed.

#### 2. WebAssembly (WASM) and Shared Memory for Multithreading

- To optimize concurrency, **WebAssembly** (WASM) with **shared memory** via **SharedArrayBuffer** can facilitate communication between **workers** (background threads) efficiently, though interaction with the main thread remains restricted.
- Lock-free operations and avoiding **atomic wait instructions** in WebAssembly are necessary, but **spin locks** (a technique where a thread repeatedly checks for a condition) are less desirable due to potential performance costs.

**Proposed Solution**:

- For communication between the **main thread** and **worker threads**, a lock-free **channel** could be implemented. For worker-to-main communication, a **polling interval with a timeout** might be necessary to avoid blocking the main thread.

---

### Future Work and Implementation Plan

1. **Editable Components**:
    
    - Implement the function to update field values and ensure a mechanism is in place to handle **concurrent user edits**.
    - Develop React components to support editing interfaces.
2. **Embedding**:
    
    - Extend the API to support embedding by line numbers and other elements beyond sections and tagged blocks.
3. **Performance Optimization**:
    
    - Investigate the root cause of idle time during initial indexing, perhaps by employing **profiling tools** or better scheduling for concurrent tasks.
4. **Universal Visual Components**:
    
    - Develop reusable UI components like buttons and dropdowns that can be utilized across different views, ensuring consistent and interactive UI experiences.
5. **Pre-built Views**:
    
    - Prioritize lightweight views such as tables and galleries, and explore chart views while considering the trade-offs of including external dependencies.
6. **Query Language Improvements**:
    
    - Implement the **infix functions** and **in operator** to enhance the query syntax and usability.

By focusing on these aspects, Datacore can evolve into a more user-friendly, high-performance, and feature-rich document query system.








### Problem and Solution Overview

#### Problem Summary:

The issue arises from the **slow runtime** during script imports, where pauses between every single import create inefficiency. Additionally, recursive script loading (via `dc.require()`) and cyclical dependencies need careful handling to avoid issues like race conditions or infinite loops. Furthermore, caching and persistent rendering are important considerations in **Datacore** for performance optimization and correct handling of file changes.

#### Proposed Solutions and Considerations:

1. **Batching Imports**:
    
    - **Batching imports in groups of 8-16** could significantly speed up the import process. The current slowdown is due to the runtime inserting pauses between each import.
2. **WebAssembly (WASM) with Shared Memory**:
    
    - **WASM and SharedArrayBuffer** can be used to communicate between **worker threads** efficiently, though direct communication with the **main thread** is restricted, as waiting on the main thread is not allowed.
    - For **main-to-worker communication**, a lock-free approach is necessary to avoid halting the main thread. One option is using **spin locks**, but this is suboptimal as it introduces busy-waiting.
    - For **worker-to-main communication**, polling with a timeout interval may be necessary to check for changes without blocking the main thread.

#### Lock-Free WASM Communication Design:

- For lock-free communication:
    1. **Main Thread to Worker**: Use a lock-free, wait-free channel to send information from the main thread to the worker.
    2. **Worker to Main Thread**: Implement a polling mechanism in the main thread that checks the worker's state periodically, with a timeout mechanism to avoid excessive polling overhead.

### Recursion and Circular Dependency Handling in `dc.require()`

#### Problem:

- When scripts recursively load each other using `dc.require()`, there is a potential for **circular dependencies**, where Script A loads Script B, and Script B loads Script A, causing a loop.
- There is also the issue where multiple scripts might load the same dependency simultaneously, leading to race conditions or multiple redundant loads.

#### Solution:

- **Proxy Object with Caching**:
    - Modify `dc.require()` to return a **JavaScript Proxy object** that intercepts the loading process. This proxy object can check a cache to see if a script is already in the process of being loaded or has already been loaded.
    - The cache will prevent the same script from being loaded multiple times concurrently.
- **ScriptCache Usage**:
    - You can bring back the `ScriptCache` introduced in [this commit](https://github.com/blacksmithgu/datacore/commit/866b5be02b0eeea3537739c4f2416da50121c193) to store information about which scripts are being loaded.
    - Use the cache to track loading status and avoid redundant or cyclic loads.
    - The `LOADING_SENTINEL` symbol tracks scripts that are currently in the process of being loaded, helping to avoid recursive loading issues.

### Caching and Persistent Rendering in Datacore

#### Consideration:

- **Datacore's persistent render model** means that scripts are executed once, and React is used to handle subsequent changes to file metadata. This differs from **Dataview**, which re-executes scripts every time.
- To optimize performance and correctly handle updates, the script should **rerun** itself if any of its dependent script files change, avoiding the need for a full reload.

#### Storing the Cache:

- The caching for loaded scripts only needs to be done per view, so it should be stored inside `DatacoreLocalApi`.
- Alternatively, you could store the cache in `DatacoreJSRenderer`, but keeping it within the API layer might be the most straightforward approach.

### Additional Notes

1. **Loading Sentinel**:
    - The `LOADING_SENTINEL` symbol is necessary for tracking scripts that are in the middle of loading. This ensures that any script requiring a dependency that is still being loaded waits for its completion.
2. **File Type and Location**:
    - The syntax for `dc.require()` and acceptable file types can be found in the `docs/javascript.md` file. Typically, files do not need to be located in the root folder but should adhere to the file type and directory structure defined in the documentation.

---

### Conclusion and Implementation Steps:

1. **Batch Imports**: Implement batching for script imports (8-16 scripts per batch) to improve performance.
2. **WASM Communication**: Use lock-free communication between the main thread and workers using SharedArrayBuffer, and implement polling for worker-to-main communication.
3. **Proxy Object for Caching**: Enhance `dc.require()` to return a Proxy object, using a cache to manage recursive or concurrent loads.
4. **ScriptCache and Caching Location**: Store loaded script info in `DatacoreLocalApi` for each view, using `ScriptCache` to handle loaded/ongoing loads.
5. **Handle Circular Dependencies**: Leverage `LOADING_SENTINEL` to track loading states and avoid circular loading.
6. **Rerun Scripts on Dependency Changes**: Ensure that scripts rerun automatically when dependencies are updated, aligning with Datacore’s persistent render model.







### Problem: Slow `IndexedDb` Performance

You’re observing that `IndexedDb` (which is being used to cache file metadata) is performing much slower than expected—taking **2-3 seconds** to load metadata for some documents. The target performance should ideally be **~1 ms per document**, but the current performance is far slower. Interestingly, **Dataview**, which also uses `IndexedDb`, does not exhibit this latency issue, raising questions about the specific usage or configuration that could be causing this slowdown in Datacore.

---

### Diagnosis and Potential Solutions

#### 1. **Check Indexing Configuration**

- One possible reason for the slow `IndexedDb` performance could be how the **indices** are set up. Dataview may be indexing data differently, optimizing retrieval times for queries.
- Ensure that the appropriate fields are being **indexed** and that the indices are optimized for the types of queries being performed.

**Action**:

- Review how **Dataview** is structuring its indices in `IndexedDb` and compare it to Datacore’s current implementation.
- Try adding or adjusting indices in Datacore to focus on the frequently queried fields to improve lookup times.

#### 2. **Bulk Retrieval vs. Individual Document Access**

- Another reason could be whether Datacore is trying to **retrieve metadata for multiple documents** at once or whether it retrieves them one by one. Bulk retrieval is often faster than retrieving data item by item in IndexedDb.

**Action**:

- If you aren’t already doing so, attempt **batch querying** documents rather than retrieving them individually. IndexedDb can perform better when handling bulk operations.

#### 3. **Storage Size and Serialization Overhead**

- The size of the metadata being stored could also play a role. Large or deeply nested objects may increase **serialization/deserialization times**, particularly if you are storing complex structures.
- `IndexedDb` stores data as **JSON-like objects**, and larger objects or more deeply nested data structures can take significantly longer to load.

**Action**:

- Investigate the structure and size of the metadata stored in `IndexedDb`. If objects are too large or contain unnecessary details, you could optimize the data structure to make it more lightweight.
- Consider **reducing nested objects** and flattening data structures for quicker retrieval.

#### 4. **IndexedDb Transaction Management**

- The way transactions are managed can heavily impact performance. IndexedDb operates using transactions, and certain configurations (like having **long-running transactions** or poor handling of transactions) can result in delays.

**Action**:

- Ensure that **transactions** are being used efficiently. Avoid long-running transactions, and group multiple operations under a single transaction if possible.
- Investigate whether you’re opening and closing multiple transactions unnecessarily, as this could add overhead.

#### 5. **Storage Mechanism Comparison**

- Dataview and Datacore may be using different **storage mechanisms** within `IndexedDb` (e.g., `localForage` or custom wrappers). Dataview may be using a lighter abstraction, resulting in faster retrieval.

**Action**:

- Compare the storage mechanism and the libraries being used by Dataview and Datacore. If Dataview uses a simpler or more efficient abstraction, consider switching or optimizing how metadata is stored and retrieved in Datacore.

#### 6. **Async Operations and Await Delays**

- Sometimes performance issues arise from **unnecessary async operations** or delayed `await` calls, where execution pauses between operations that could otherwise be grouped.

**Action**:

- Review the use of `async`/`await` and ensure there aren’t unnecessary pauses between operations. **Debounce** or **throttle** any operations that don’t need to be immediately executed to reduce perceived latency.

---

### Solution for Editing in Table Views

#### Problem:

When implementing table view editing, the goal is to design a system that allows users to **edit specific fields** in a straightforward manner without getting bogged down by complex or edge cases (such as handling null values or varying data types). The challenge lies in detecting what field the user is trying to edit and updating the corresponding field/item in the data model correctly.

#### Proposed Approach:

The simplest and cleanest solution is to build **specific components** that handle updating **one defined field** at a time. This avoids complications with dynamic field detection or handling inconsistencies in data types across pages.

#### Solution Design:

1. **Editable Columns**:
    
    - Define columns with specific fields that are **editable** by the user. Each editable column will specify:
        - The field it is tied to.
        - How to render the field’s editor (e.g., a slider for a rating, a text box for a text field).
2. **Field Abstraction**:
    
    - Utilize the existing `Field` abstraction in Datacore. This abstraction already contains the necessary **metadata** about the field’s type and origin. You can pass this metadata to the editor component to handle the editing process appropriately.
3. **Code Example**:
    

```javascript
const EDITABLE_COLUMN = { 
  id: "example-editable",
  value: page => page.value("rating"),
  render: (rating, page) => <dc.EditableSlider field={page.field("rating")} ... />
};
````

- In this example:
    - `id`: A unique identifier for the editable column.
    - `value`: A function that extracts the current value for the field (in this case, `rating`).
    - `render`: A function that renders an editable component (`EditableSlider`), passing in the `rating` field and page information.

4. **Component-Specific Editors**:
    - Build different editor components for specific field types (e.g., sliders for numeric values, text boxes for strings, date pickers for dates).
    - Example components:
        - **Textbox/TextArea**: For editing text fields.
        - **Slider/Rating**: For editing numeric fields.
        - **Date Picker**: For editing date fields (perhaps using the default Obsidian date picker).

#### Why This Approach?

- **Avoids Ambiguity**: Forcing the user to specify how the field should be edited avoids complex logic to detect field types automatically, which could introduce bugs (e.g., handling null values or fields with inconsistent types across documents).
- **Solid Foundation**: This manual approach creates a solid foundation for editing, which can be expanded later to support more advanced or automatic editing.

---

### Conclusion:

1. **IndexedDb Performance**:
    
    - Investigate the use of indices, bulk retrieval, metadata size, and transaction management to address slow IndexedDb performance in Datacore. Compare with Dataview’s implementation to pinpoint differences.
2. **Editing in Table Views**:
    
    - Implement specific components that allow users to edit individual fields in table views using the `Field` abstraction. This approach is simple, avoids complex issues, and provides a solid starting point for expanding editing functionality in the future.








### Explanation of the Data Structure and Rendering

#### Overview

The underlying data in this setup is a **recursively grouped set of tasks**, which means that each task can have sub-tasks (or elements) nested within it. This recursive nature allows the data to be represented in a **tree-like structure**, where each task (or group) may contain other tasks as children.

You’ve created a function to handle this recursive structure called `groupFn`. Additionally, a boolean property (`displayAsRow`) in the configuration dictates whether each group should be rendered as a **header** or a **regular table row**. The rendering of this tree data in the UI occurs in components like `VanillaRowGroup`, `TableRow`, and `TableGroupHeader`.

---

### Code Breakdown

#### 1. **Data Grouping Function: `groupFn`**

This function is responsible for recursively traversing the data and grouping tasks:

```js
const groupFn = x => {
    let variable = x.$elements || [];
    return {
        key: x.key || x,
        rows: variable.map(groupFn)
    }
}
```

**Global Functionality**:

- The `groupFn` function receives a task object `x` and checks if it contains an `$elements` property, which holds sub-tasks. If it does, it recursively maps through these sub-tasks using `groupFn` again, continuing the recursive grouping process.
- It returns an object with:
    - **`key`**: The task identifier (used for rendering).
    - **`rows`**: The sub-tasks, recursively processed by the same function.

**Explanation**:

- This function essentially builds a **tree structure** from a flat list of tasks, where each task can have its own children (sub-tasks).
- The recursion ensures that each task is grouped with its sub-tasks, so the data can later be rendered hierarchically (as a nested structure).

---

#### 2. **`VanillaRowGroup` Rendering Based on `displayAsRow`**

In the `VanillaRowGroup` component, you've implemented logic that renders either a **table row** or a **group header** depending on the `displayAsRow` flag:

```js
groupingConfig?.displayAsRow ? (
    <TableRow
        open={open}
        openChanged={setOpen}
        row={element.key as T}
        columns={columns}
        level={level}
        hasChildren={element.rows.length > 0}
    />
) : (
    <TableGroupHeader
        level={level}
        value={element}
        width={columns.length}
        config={groupingConfig}
        open={open}
        openChanged={setOpen}
    />
);
````

**Global Functionality**:

- Based on the **`displayAsRow`** configuration, this section either:
    - **Renders a table row** (`<TableRow />`) for regular tasks if `displayAsRow` is `true`.
    - **Renders a group header** (`<TableGroupHeader />`) for grouped tasks if `displayAsRow` is `false`.

**Explanation**:

- This mechanism allows for flexible rendering of grouped data. If the `displayAsRow` flag is set, a normal task is rendered in the table, otherwise, it’s treated as a **group** with possible sub-tasks nested underneath.

---

#### 3. **Updated `TableRow` Component**

The `TableRow` component is responsible for rendering an individual row in the table, including handling indentation based on the task’s depth (or level) in the tree structure:


```js

export function TableRow<T>({
    level,
    row,
    columns,
    openChanged,
    open,
    hasChildren = false,
}: {
    level: number;
    row: T;
    columns: VanillaColumn<T>[];
    openChanged: (b: boolean) => void;
    open: boolean;
    hasChildren?: boolean;
}) {
    return (
        <tr className="datacore-table-row" data-level={level}>
            <td style={level ? `padding-left: ${level * 25}px;` : undefined}>
                {hasChildren ? <TableCollapser open={open} openChanged={openChanged} /> : null}
            </td>
            {columns.map((col) => (
                <TableRowCell row={row} column={col} level={level} />
            ))}
        </tr>
    );
}
```

**Global Functionality**:

- The `TableRow` component renders a table row with data, managing the following:
    - **`level`**: Indentation based on the depth of the task in the hierarchy (calculated as `padding-left`).
    - **`openChanged`**: Callback to toggle whether child rows are visible (used in case there are sub-tasks).
    - **`hasChildren`**: Checks whether the row has sub-tasks and, if so, renders a **collapser** button (`<TableCollapser />`).
    - **`columns.map(...)`**: Iterates through columns and renders the appropriate content for each cell.

**Explanation**:

- This component ensures that each task is rendered with the proper indentation depending on its **level** in the hierarchy, making the tree structure visually clear.
- If a task has sub-tasks, it adds a collapsible button to expand or collapse the children.

---

### Summary of Rendering Tree-Like Data in Table

1. **Recursive Data Grouping**:
    
    - The `groupFn` function organizes the data into a tree-like structure, where each task may have children (sub-tasks) grouped recursively.
2. **Conditional Rendering Based on `displayAsRow`**:
    
    - Depending on the **`displayAsRow`** flag, tasks are either rendered as normal table rows (`TableRow`) or as group headers (`TableGroupHeader`).
3. **Hierarchical Row Indentation**:
    
    - Each task’s **level** in the hierarchy is used to control its **indentation** (via padding) within the table, and if it has sub-tasks, a collapsible control is provided.
4. **Data-Level for Debugging**:
    
    - The `data-level` attribute is used in `TableRow` for debugging, indicating the row’s depth in the tree.

This flexible system allows for rendering hierarchical data (tasks and sub-tasks) in a structured and visually distinct manner, with components designed to handle dynamic content based on configuration options.













### Addressing `open` / `setOpen` Behavior and State Persistence

#### Question Recap:

You’re concerned about how `open` / `setOpen` works in the context of table rows, especially for handling deep nesting and how left padding might impact the layout. Additionally, React does not inherently preserve the state (whether rows are collapsed or expanded) across page navigation, meaning that when you move between pages and return, the **open state** of the rows will be lost. To mitigate this, you’ll need a way to track the open state at a higher level.

---

### Problem: Preserving Row Open/Closed State Across Pages

#### Current Setup:

- The `open` state is handled at the row level to control collapsing/expanding of table rows.
- While this works fine within the current page, the state is **lost** when navigating between pages since React does not automatically persist the state across different page renders.

#### Proposed Solution:

1. **Track State at the Top Level**:
    
    - The `open` state for each row should be managed at the **top level of the view**, outside of the table itself. This way, the state is **global** and persists across navigation.
    - Each row will need a **unique identifier** so the state can be tracked individually for each row.
2. **Assigning Unique Identifiers**:
    
    - The **best solution** is to use a unique identifier for each row, such as the `$id` property, but this only works if all rows have a unique `$id`. In cases where rows don’t have `$id` or multiple rows share the same one, this solution won’t be reliable.
    - An alternative is using the **index** of the row in the data. However, the downside is that sorting or new data loads can change the order of rows, leading to incorrect open/closed states.
    
    **Ideal Approach**:
    
    - If possible, rely on a **persistent identifier**, like `$id`, which remains unchanged even when sorting or reloading.
    - If `$id` isn’t feasible across all cases, use a combination of the row’s position and data content (e.g., a hashed version of the row’s data) to generate a stable unique key.

---

### Code Example: Tracking Open State with Unique IDs

Here’s an example of how you could modify the existing code to track the **open/closed state** at a higher level using row IDs:

1. **Managing Open State at the Top Level**:

```js
const [openRows, setOpenRows] = useState<{ [key: string]: boolean }>({});

const toggleRowOpen = (id: string) => {
    setOpenRows(prev => ({ ...prev, [id]: !prev[id] }));
};
```

- `openRows` is an object where the **key** is the unique identifier of the row (e.g., `$id`) and the **value** is whether the row is open (`true`) or closed (`false`).
- `toggleRowOpen` toggles the open state for a specific row.

2. **Passing Open State to Rows**:

In your `TableRow` component, you would now pass the `open` state and a function to toggle it based on the unique ID of the row:

```js
<TableRow
    open={openRows[element.$id]}
    openChanged={() => toggleRowOpen(element.$id)}
    row={element.key as T}
    columns={columns}
    level={level}
    hasChildren={element.rows.length > 0}
/>
```

- **`element.$id`** is used to identify the row. When the row is toggled, the `toggleRowOpen` function updates the open state in `openRows`.

---

### Benefits of Using Persistent IDs

Using unique, persistent IDs like `$id` ensures that the state of rows persists reliably across various operations (sorting, filtering, etc.) and between page transitions. This solves the issue where row state would otherwise be lost when navigating between pages or re-rendering.

---

### Editable Text Column Implementation

#### Existing Text Column:

The text column you’ve implemented looks as follows:

```js
{
    id: "texto",
    title: "text",
    value: x => x.$cleanText,
    editable: true,
    onUpdate: (v, x) => dc.setTaskText(v, x),
    editor: (v, o, d) => <dc.TextEditor dispatch={d} text={v} inline={false}/>,
    render: (v, o) => <dc.Markdown content={v} inline/>
}
```

**Breakdown**:

- **`id`**: Column identifier (`"texto"`).
- **`title`**: The title of the column, shown as the header.
- **`value`**: Extracts the clean text (`$cleanText`) from the row data.
- **`editable`**: Marks the column as editable.
- **`onUpdate`**: The function that gets called when the user updates the text (using `dc.setTaskText(v, x)`).
- **`editor`**: The custom editor used for this column (in this case, a `TextEditor` component).
- **`render`**: Renders the value (as Markdown in this case) for display in the column.

---

### Next Steps: Canvas Indexing & PDFs

1. **Canvas Indexing**:
    - You can go ahead and start working on **canvas indexing** if you'd like. Since you’ve already finished the editing implementation for the table, canvas could be the next logical step.
2. **PDF Indexing**:
    - As you noticed, PDF indexing was **temporarily removed** due to breaking issues that couldn’t be debugged in time for a release. You might revisit this after addressing canvas indexing, depending on the priorities. The broken functionality will need to be debugged and re-implemented after identifying the underlying issues.

---

### Conclusion

1. **Preserving Open/Closed State**: Move the `open` state management to the top level of the view and track it using a unique identifier (preferably `$id`). This ensures the state is preserved across page transitions and data reloads.
    
2. **Editable Text Column**: Your current implementation for the editable text column is solid, utilizing a custom `TextEditor` for inline edits and Markdown for display.
    
3. **Next Steps**: Start working on **canvas indexing** and plan to revisit the PDF indexing later.










### Breakdown of PDF Indexing Issues

#### Problem Summary:

- The **PDF indexing** functionality was broken and subsequently disabled due to two main issues:
    1. The **Web Worker** was pulling the **PDF.js** distribution directly from a CDN, causing failures when offline. It was suggested to either use Obsidian's bundled PDF.js or bundle the library ourselves.
    2. Parsing of **every PDF file** was failing silently, and no clear error message was available, making debugging difficult.

---

### Key Issues and Discussion Points

#### 1. **PDF.js Dependency and Web Worker Challenges**:

- **Issue with CDN Dependency**:
    
    - The initial implementation had the Web Worker pulling **PDF.js** from a CDN, which would break when offline.
    - This is not a reliable approach because **external CDN dependencies** can fail if the user is offline or if the CDN experiences downtime.
- **Proposed Solutions**:
    
    - **Option 1**: Use the **PDF.js** that comes with **Obsidian**. However, this is not feasible due to the limitation where **global variables declared in the main thread are not available in workers**. This would prevent direct access to the PDF.js library from the Web Worker.
    - **Option 2**: **Bundle PDF.js** into the build, ensuring that it is always available, whether online or offline. The downside is that this could increase the size of the **main.js** file.
- **Bundle Size Concern**:
    
    - The **npm package size** of PDF.js is large, but the production bundle (minified) is only around **3 MB**. This is still substantial, but given the processing power of PDF.js, it might be an acceptable trade-off.
    - Despite the size, PDF.js uses a **worker internally** for heavy PDF processing, so performance might not take a serious hit on the main thread even if we process PDFs there.

---

#### 2. **Silent Parsing Failures**:

- **Parsing Failure Without Errors**:
    
    - Another major issue was that **every PDF file** was failing to parse without any meaningful error message. This could be caused by several factors, including:
        - Incorrect use of the PDF.js API.
        - Incompatibility with certain PDF structures.
        - Miscommunication between the worker and the main thread.
- **Suggested Debugging Steps**:
    
    - Capture **detailed error messages** during the PDF parsing process. Silent failures are often due to exceptions being swallowed, so improving logging or adding `try/catch` blocks with explicit error messages could help identify where the issue lies.
    - **Test specific PDFs** to identify if the failure is consistent across all files or if it occurs only with certain types of PDFs. This might reveal edge cases that need to be handled.

---

### Decision Points

#### Should We Bundle PDF.js in the Main Build?

- **Benefits**:
    
    - **Offline support**: By bundling PDF.js directly into the main application, the Web Worker will no longer depend on an external CDN. This ensures that PDF parsing works whether the user is online or offline.
    - **Consistency**: Bundling ensures that the same version of PDF.js is used across environments, reducing potential issues related to mismatched versions or CDN unavailability.
- **Drawbacks**:
    
    - **File size**: Bundling PDF.js will add approximately 3 MB to the production build. This is not negligible, but if PDF indexing is an important feature, it might be a necessary trade-off.
    
    Given that the **main thread** is not directly processing the PDF (since PDF.js uses its own worker), performance may not take a significant hit.
    

#### Alternative: Handle PDFs in the Main Thread

- **Main Thread Processing**:
    - One alternative solution is to handle PDF processing directly on the **main thread**, but this is less ideal since PDF.js was designed to offload processing to a worker for performance reasons.
    - Moving everything to the main thread could **hurt performance**, especially for larger PDFs or if multiple PDFs are being indexed simultaneously.

#### PDF Metadata Consideration

- **Metadata** Value:
    - PDFs typically contain limited metadata, so indexing PDFs for metadata retrieval might not be worth the added complexity and size of the PDF.js library.
    - For example, the most useful metadata might be the **number of pages**, which could potentially be extracted using a simpler, custom PDF parsing solution instead of relying on the full PDF.js package.

---

### Proposed Next Steps

1. **PDF.js Bundling**:
    
    - Proceed with **bundling PDF.js** into the main build. This will solve the issue of relying on external CDNs and ensure consistent parsing behavior.
    - Use **minified production builds** of PDF.js to reduce the overall size.
2. **Improve Error Handling**:
    
    - Enhance error logging around the PDF parsing process. Add `try/catch` blocks and surface meaningful error messages so that parsing failures can be properly debugged.
    - Investigate the **silent failures** by testing specific PDF files and analyzing where and why the parsing is breaking.
3. **Explore Lightweight PDF Parsing**:
    
    - If PDF indexing is only needed for minimal metadata like **page count**, consider developing a **lightweight, custom PDF parser** that can extract this information without requiring the full PDF.js library. This could dramatically reduce the bundle size and simplify the indexing process.

---

### Additional Notes on CI/CD Setup

- You mentioned the repo doesn't have any **CI/CD** setup at present. It would be useful to set up continuous integration for formatting, linting, and code checks. Adding CI/CD ensures that all changes (like the PDF.js bundling or PDF parsing improvements) can be automatically validated, reducing the risk of bugs.
- This could include:
    - **Linting checks** for code quality.
    - **Automated tests** for critical components (such as the PDF parsing function) to catch issues early.

---

### Conclusion

- **Bundle PDF.js** to ensure reliable and consistent PDF parsing, with offline functionality.
- **Improve error handling** to debug the silent failures, and test PDFs to identify specific parsing issues.
- Consider a **lightweight custom PDF parser** for extracting basic metadata if full PDF.js functionality isn’t necessary.
- Set up a **CI/CD pipeline** for the project to ensure future changes are checked for consistency and quality.





### Problem Summary

The PDF indexing feature was disabled due to multiple issues during development, which could not be debugged in time for release. The problem involved several factors, including issues with using `pdfjs` and the integration of web workers for parsing PDFs.

### Global Functionality

The overall discussion centers on managing the PDF indexing functionality within an application. The key concern is the handling of PDF parsing through `pdfjs`, especially in an offline context and how to avoid relying on a CDN for the library. Additionally, the question of bundling the `pdfjs` library into the main `JavaScript` file, with concerns about the size and performance impacts, is addressed.

### Problem Breakdown

1. **Web Worker and CDN Issue**:
    
    - **Problem**: The web worker responsible for handling PDF parsing was pulling the `pdfjs` library from a Content Delivery Network (CDN). When the system went offline, this caused the parsing process to break.
    - **Solution Suggestion**: Avoid using a CDN for `pdfjs` and either:
        1. Use the version of `pdfjs` bundled with Obsidian, or
        2. Bundle `pdfjs` directly with the main JavaScript file in the project.
2. **PDF Parsing Failure**:
    
    - **Problem**: There were silent failures when trying to parse PDFs via `pdfjs`, and every file failed. The exact error was not captured or remembered at the time.
    - **Possible Cause**: It could be related to the way the web worker was handling the `pdfjs` library or an issue with the offline environment where the CDN was unavailable.
3. **Bundling Concerns**:
    
    - **Question**: Would the final `main.js` size be acceptable if `pdfjs` were bundled into the project?
    - **Response**: The production build of `pdfjs` is only about 3MB, despite its larger size with source maps, so the size might not be a major concern.
4. **Web Worker Limitation**:
    
    - **Issue**: The suggestion to use the version of `pdfjs` that comes with Obsidian was dismissed due to technical limitations. Specifically, global variables declared in the main thread are not available in web workers, complicating the implementation.
    - **Solution Consideration**: A workaround is to handle PDFs directly on the main thread, but this could potentially affect performance negatively, as PDF processing in `pdfjs` typically occurs outside the main thread in a worker for performance reasons.
5. **Optimizing PDF Metadata Parsing**:
    
    - **Observation**: PDF metadata often lacks useful information, making the heavy processing burden of `pdfjs` seem unnecessary if the goal is only to retrieve simple details like the number of pages.
    - **Potential Improvement**: Write a lightweight custom PDF parsing solution to retrieve essential metadata like the page count, potentially reducing the size and complexity of the required library.

### Additional Context and Suggestions

1. **Continuous Integration (CI) Setup**:
    - The repository does not currently have any CI/CD setup for automatic linting, formatting, and other checks.
    - **Solution**: MRs (Merge Requests) to add CI/CD would be appreciated to help maintain code quality.

### Possible Next Steps:

1. **Bundle `pdfjs` Locally**:
    
    - Since downloading from a CDN can cause issues, it might be better to bundle `pdfjs` with the application directly. The production bundle size is relatively small (3MB), so performance or file size concerns may be manageable.
2. **Optimize PDF Parsing**:
    
    - Depending on the specific metadata needed from PDFs, consider creating a lightweight parsing method that only retrieves essential information (like page count), bypassing the need for a large library like `pdfjs`.
3. **Handle PDF Parsing on the Main Thread (with Caution)**:
    
    - If bundling `pdfjs` with a worker proves problematic, try handling the parsing on the main thread, but monitor performance impacts closely. Since `pdfjs` already uses a worker internally, it’s best to weigh the performance trade-offs before fully switching to the main thread.


1. **CI/CD Integration**:
    
    - Setting up a continuous integration pipeline will improve the maintainability of the project. This will allow for automated formatting, linting, and testing checks during development.





### Problem Summary

This conversation revolves around two main issues:

1. **Rendering Inline Fields in Obsidian**:
    
    - There is a problem with displaying multiple inline fields in Obsidian if they are not separated by at least two spaces. This causes the fields to only show up when the cursor is placed over them.
    - The suggestion is to place a comma `,` between the fields, as this aligns with Markdown's behavior, where `[ ][ ]` can be interpreted as reference-style links.
2. **Switching Persistence Layer in Datacore**:
    
    - **Proposal**: There's a suggestion to move from using `IndexedDB` as the persistence layer for Datacore to `SQLite`. This transition would allow Datacore to work seamlessly on both mobile and desktop versions using SQLite's `OPFS` (Origin Private File System) implementation.
    - **Discussion**: Concerns revolve around the performance implications of such a switch, especially considering the overhead of using SQLite with WebAssembly (WASM) in browsers. There's a need to benchmark and compare the performance of `IndexedDB` and SQLite (using both plain OPFS and `SyncAccessHandle` OPFS) to make a decision.

### Detailed Breakdown

#### 1. **Inline Fields in Obsidian**

- **Problem**: Multiple inline fields do not display correctly unless the user hovers over them with the cursor. This happens when fields like `[[ ]]` are placed too close together.
- **Solution**: The suggestion is to insert a comma (`,`) between the inline fields, as this is a Markdown formatting issue. Without a delimiter like a comma, Markdown interprets closely placed brackets as reference-style links, causing rendering issues.

#### 2. **Persistence Layer for Datacore**

- **Current Setup**: Datacore uses `IndexedDB` for data persistence. While this works well, there are concerns about its performance, especially when dealing with very large vaults (10,000+ notes).
- **Proposed Change**: Switch to SQLite with an OPFS-backed VFS (virtual file system) for both mobile and desktop.
    - SQLite with OPFS offers more flexibility and is potentially faster, but benchmarks need to be run to assess performance.
    - The major difference would be in how data is stored and retrieved, with SQLite potentially offering better consistency and performance under certain workloads.
- **Benchmarking**: Two specific benchmarks will be conducted:
    1. **IndexedDB vs. Plain OPFS VFS**: This benchmark will compare the default use of SQLite with OPFS against IndexedDB.
    2. **IndexedDB vs. `SyncAccessHandle` OPFS VFS**: `SyncAccessHandle` provides optimized read/write access and could improve SQLite's performance further.
- **WASM Overhead**: There is concern about the performance penalty introduced by using SQLite in WebAssembly (WASM), especially in Firefox, which may suffer more from this overhead compared to Chromium-based browsers that use `LevelDB` (the underlying storage for IndexedDB).

#### Key Considerations for Switching to SQLite

- **Memory Usage**: While both IndexedDB and SQLite can handle large datasets, the switch to SQLite could be beneficial if Datacore needs to scale to even larger vaults.
- **Cross-Platform**: SQLite can be used consistently across mobile and desktop platforms, which is a key advantage. The SQLite OPFS implementation ensures that file system access works reliably on both platforms.
- **WASM Overhead**: The performance hit from using WebAssembly to run SQLite in the browser may be a deciding factor. This overhead is typically more prominent in non-Chromium browsers like Firefox.
- **Benchmarking Results**: The decision to switch will depend on the results of performance benchmarks, which will compare IndexedDB and SQLite under various conditions.

### Conclusion and Next Steps

1. **Inline Field Rendering**: The Markdown issue with inline fields can be resolved by ensuring there is a comma or adequate spacing between them to prevent misinterpretation as reference links.
    
2. **Persistence Layer Evaluation**:
    
    - Benchmark tests should be conducted to compare the performance of `IndexedDB` and `SQLite` (with plain OPFS and `SyncAccessHandle` OPFS).
    - Analyze the results to determine if SQLite provides significant performance gains over `IndexedDB`, especially for large datasets in Datacore.
    - Consider the trade-offs in terms of WASM overhead, particularly in non-Chromium browsers, before making a final decision on whether to switch to SQLite.